{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Embedding extraction through VQA without images\n",
    "\n",
    "In this notebook we're going to use a Recursive Neural Network for generating Question embeddings based on the question format and it's answer.\n",
    "\n",
    "The goal of this is to extract array representation for the questions where similar questions are close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- question: string (nullable = true)\n",
      " |-- answer: string (nullable = true)\n",
      " |-- image_id: string (nullable = true)\n",
      " |-- tokenized_question: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- question_len: double (nullable = true)\n",
      " |-- question_word_len: double (nullable = true)\n",
      " |-- first_word: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1895874"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"QuestionRephrasing-AutoEncoder\") \\\n",
    "    .config(\"spark.executor.memory\", \"14G\")\\\n",
    "    .config(\"spark.driver.memory\", \"14G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"14G\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('data/checkpoints')\n",
    "questions = spark.read.parquet(\"data/processed/union/*\")\n",
    "questions.printSchema()\n",
    "questions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question filtering based on answer frequencies\n",
    "\n",
    "we're dealing with a dataset that has a wide variety of answers. So for this we'll consider only the answers that appear often in order to reduce the classes predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We're going to target only 332 answers\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_counts = questions.select('answer')\\\n",
    "    .toPandas()['answer']\\\n",
    "    .value_counts()\n",
    "answers_counts = pd.DataFrame(list(zip(answers_counts.keys(), answers_counts.values)))\n",
    "most_common_answers = answers_counts[answers_counts[1] > 600][0]\n",
    "f\"We're going to target only {most_common_answers.count()} answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_answers_df = spark.createDataFrame([[x] for x in most_common_answers.tolist()], [\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question size: 1895874, after filtering 1038866\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original question size: {questions.count()}, after filtering {questions.join(most_common_answers_df, 'answer').count()}\")\n",
    "questions = questions.join(most_common_answers_df, 'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mmaximum number of word in a question is 28.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_len = int(questions.agg({\"question_word_len\": \"max\"}).collect()[0][\"max(question_word_len)\"])\n",
    "\n",
    "f\"Mmaximum number of word in a question is {max_word_len}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Word mapping example for 'is': 15.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens vocabulary and mappers\n",
    "tokens = questions.select('tokenized_question')\\\n",
    "    .rdd\\\n",
    "    .flatMap(lambda x: x['tokenized_question'])\\\n",
    "    .collect()\n",
    "\n",
    "word_mapping = {}\n",
    "word_mapping_reversed = {}\n",
    "word_counter = Counter(tokens)\n",
    "for idx, value in enumerate(word_counter):\n",
    "    word_mapping[value] = idx\n",
    "    word_mapping_reversed[idx] = value\n",
    "    \n",
    "f\"Word mapping example for 'is': {word_mapping['is']}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the word representation of input using the mapping above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(answer='plane', question=\"what's in the sky?\", image_id='21926', tokenized_question=['what', \"'s\", 'in', 'the', 'sky', '?'], question_len=18.0, question_word_len=6.0, first_word='what', question_word_embeddings=[1, 2, 3, 4, 5, 6])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_word_embeddings = F.udf(lambda tokenized_question: [word_mapping[word] + 1 for word in tokenized_question], ArrayType(IntegerType()))\n",
    "\n",
    "questions = questions.withColumn('question_word_embeddings', extract_word_embeddings(F.col('tokenized_question')))\n",
    "questions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = questions.select('question_word_embeddings')\\\n",
    "    .rdd\\\n",
    "    .map(lambda x: x['question_word_embeddings'])\\\n",
    "    .collect()\n",
    "word_embeddings = pad_sequences(word_embeddings, maxlen=max_word_len, dtype='int32', padding='post', truncating='pre', value=0.0)\n",
    "word_embeddings[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the answers targets to one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x332 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens = questions.select('answer')\\\n",
    "    .rdd\\\n",
    "    .map(lambda x: x['answer'])\\\n",
    "    .collect()\n",
    "\n",
    "nr_answers = len(set(answer_tokens))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(answer_tokens)\n",
    "onehot_encoder = OneHotEncoder(sparse=True)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "y[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for extracting question embeddings\n",
    "\n",
    "**Input:** question embedded (each token represents a number according to the above mapping) as sequences and padded\n",
    "\n",
    "**Output:** answers as one-hot\n",
    "\n",
    "For extracting the question embeddings, we'll train the model and then we'll drop the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 28, 50)            840850    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               120800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 332)               16932     \n",
      "=================================================================\n",
      "Total params: 988,632\n",
      "Trainable params: 988,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_mapping) + 1, 50, input_length=max_word_len, mask_zero=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(100, activation='relu', dropout=0.1, recurrent_dropout=0.1), input_shape=(max_word_len, 50)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(nr_answers, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute answers class weights\n",
    "\n",
    "Since there are lots of answers the dataset is very imbalanced. Thus for optimizing the training we're providing class weights so the model can adjust based on how many times the answer appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "classes = list(set(answer_tokens))\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes, answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(zip(label_encoder.fit_transform(classes), class_weights))\n",
    "weights.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1038866/1038866 [==============================] - 542s 522us/step - loss: 2.1425 - accuracy: 0.3633\n",
      "\n",
      "Epoch 00001: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-01-0.36.hdf5\n",
      "Epoch 2/25\n",
      "1038866/1038866 [==============================] - 471s 454us/step - loss: 2.0277 - accuracy: 0.3789\n",
      "\n",
      "Epoch 00002: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-02-0.38.hdf5\n",
      "Epoch 3/25\n",
      "1038866/1038866 [==============================] - 302s 290us/step - loss: 2.0011 - accuracy: 0.3839\n",
      "\n",
      "Epoch 00003: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-03-0.38.hdf5\n",
      "Epoch 4/25\n",
      "1038866/1038866 [==============================] - 324s 312us/step - loss: 1.9812 - accuracy: 0.3883\n",
      "\n",
      "Epoch 00004: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-04-0.39.hdf5\n",
      "Epoch 5/25\n",
      "1038866/1038866 [==============================] - 329s 316us/step - loss: 1.9661 - accuracy: 0.3909\n",
      "\n",
      "Epoch 00005: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-05-0.39.hdf5\n",
      "Epoch 6/25\n",
      "1038866/1038866 [==============================] - 331s 318us/step - loss: 1.9528 - accuracy: 0.3938\n",
      "\n",
      "Epoch 00006: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-06-0.39.hdf5\n",
      "Epoch 7/25\n",
      "1038866/1038866 [==============================] - 356s 343us/step - loss: 1.9420 - accuracy: 0.3960\n",
      "\n",
      "Epoch 00007: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-07-0.40.hdf5\n",
      "Epoch 8/25\n",
      "1038866/1038866 [==============================] - 528s 508us/step - loss: 1.9321 - accuracy: 0.3983\n",
      "\n",
      "Epoch 00008: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-08-0.40.hdf5\n",
      "Epoch 9/25\n",
      "1038866/1038866 [==============================] - 563s 542us/step - loss: 1.9242 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00009: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-09-0.40.hdf5\n",
      "Epoch 10/25\n",
      "1038866/1038866 [==============================] - 563s 542us/step - loss: 1.9171 - accuracy: 0.4015\n",
      "\n",
      "Epoch 00010: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-10-0.40.hdf5\n",
      "Epoch 11/25\n",
      "1038866/1038866 [==============================] - 565s 544us/step - loss: 1.9104 - accuracy: 0.4027\n",
      "\n",
      "Epoch 00011: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-11-0.40.hdf5\n",
      "Epoch 12/25\n",
      "1038866/1038866 [==============================] - 563s 542us/step - loss: 1.9054 - accuracy: 0.4035\n",
      "\n",
      "Epoch 00012: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-12-0.40.hdf5\n",
      "Epoch 13/25\n",
      "1038866/1038866 [==============================] - 519s 500us/step - loss: 1.9001 - accuracy: 0.4044\n",
      "\n",
      "Epoch 00013: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-13-0.40.hdf5\n",
      "Epoch 14/25\n",
      "1038866/1038866 [==============================] - 490s 471us/step - loss: 1.8949 - accuracy: 0.4056\n",
      "\n",
      "Epoch 00014: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-14-0.41.hdf5\n",
      "Epoch 15/25\n",
      "1038866/1038866 [==============================] - 562s 541us/step - loss: 1.8907 - accuracy: 0.4064\n",
      "\n",
      "Epoch 00015: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-15-0.41.hdf5\n",
      "Epoch 16/25\n",
      "1038866/1038866 [==============================] - 561s 540us/step - loss: 1.8869 - accuracy: 0.4075\n",
      "\n",
      "Epoch 00016: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-16-0.41.hdf5\n",
      "Epoch 17/25\n",
      "1038866/1038866 [==============================] - 563s 542us/step - loss: 1.8820 - accuracy: 0.4077\n",
      "\n",
      "Epoch 00017: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-17-0.41.hdf5\n",
      "Epoch 18/25\n",
      "1038866/1038866 [==============================] - 564s 543us/step - loss: 1.8794 - accuracy: 0.4082\n",
      "\n",
      "Epoch 00018: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-18-0.41.hdf5\n",
      "Epoch 19/25\n",
      "1038866/1038866 [==============================] - 475s 457us/step - loss: 1.8760 - accuracy: 0.4099\n",
      "\n",
      "Epoch 00019: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-19-0.41.hdf5\n",
      "Epoch 20/25\n",
      "1038866/1038866 [==============================] - 482s 464us/step - loss: 1.8731 - accuracy: 0.4100\n",
      "\n",
      "Epoch 00020: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-20-0.41.hdf5\n",
      "Epoch 21/25\n",
      "1038866/1038866 [==============================] - 563s 542us/step - loss: 1.8711 - accuracy: 0.4111\n",
      "\n",
      "Epoch 00021: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-21-0.41.hdf5\n",
      "Epoch 22/25\n",
      "1038866/1038866 [==============================] - 561s 540us/step - loss: 1.8683 - accuracy: 0.4106\n",
      "\n",
      "Epoch 00022: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-22-0.41.hdf5\n",
      "Epoch 23/25\n",
      "1038866/1038866 [==============================] - 561s 540us/step - loss: 1.8646 - accuracy: 0.4117\n",
      "\n",
      "Epoch 00023: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-23-0.41.hdf5\n",
      "Epoch 24/25\n",
      "1038866/1038866 [==============================] - 561s 540us/step - loss: 1.8629 - accuracy: 0.4122\n",
      "\n",
      "Epoch 00024: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-24-0.41.hdf5\n",
      "Epoch 25/25\n",
      "1038866/1038866 [==============================] - 561s 540us/step - loss: 1.8602 - accuracy: 0.4126\n",
      "\n",
      "Epoch 00025: saving model to model-checkpoints/answer-embeddings/answer-embeddings-model-25-0.41.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x20bea32ed88>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"model-checkpoints/answer-embeddings/answer-embeddings-model-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(word_embeddings, [y],\n",
    "                epochs=25,\n",
    "                batch_size=1000,\n",
    "                shuffle=True,\n",
    "                callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
